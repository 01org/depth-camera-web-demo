<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=dB3GuSCq0f_LRNaO3opFPA');.lst-kix_mkjo0yq6ldhg-6>li:before{content:"\0025cf  "}.lst-kix_mkjo0yq6ldhg-4>li:before{content:"\0025cb  "}.lst-kix_mkjo0yq6ldhg-8>li:before{content:"\0025a0  "}.lst-kix_mkjo0yq6ldhg-5>li:before{content:"\0025a0  "}.lst-kix_mkjo0yq6ldhg-2>li:before{content:"\0025a0  "}.lst-kix_mkjo0yq6ldhg-3>li:before{content:"\0025cf  "}ul.lst-kix_mkjo0yq6ldhg-4{list-style-type:none}ul.lst-kix_mkjo0yq6ldhg-3{list-style-type:none}ul.lst-kix_mkjo0yq6ldhg-2{list-style-type:none}ul.lst-kix_mkjo0yq6ldhg-1{list-style-type:none}.lst-kix_mkjo0yq6ldhg-0>li:before{content:"\0025a0  "}ul.lst-kix_mkjo0yq6ldhg-8{list-style-type:none}ul.lst-kix_mkjo0yq6ldhg-7{list-style-type:none}.lst-kix_mkjo0yq6ldhg-1>li:before{content:"\0025cb  "}ul.lst-kix_mkjo0yq6ldhg-6{list-style-type:none}ul.lst-kix_mkjo0yq6ldhg-5{list-style-type:none}ul.lst-kix_mkjo0yq6ldhg-0{list-style-type:none}.lst-kix_mkjo0yq6ldhg-7>li:before{content:"\0025cb  "}ol{margin:0;padding:0}table td,table th{padding:0}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Open Sans";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Open Sans";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center;height:11pt}.c17{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c15{padding-top:10pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c20{padding-top:0pt;padding-bottom:3pt;line-height:1.0;page-break-after:avoid;text-align:left}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left;height:11pt}.c31{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c27{padding-top:20pt;padding-bottom:6pt;line-height:1.0;page-break-after:avoid;text-align:left}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:center;height:11pt}.c11{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c26{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:center}.c24{color:#666666;text-decoration:none;vertical-align:baseline;font-style:normal}.c21{padding-top:10pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c3{font-size:14pt;font-family:"Courier New";font-weight:400}.c13{font-weight:400;font-size:11pt;font-family:"Arial"}.c12{font-size:10pt;font-family:"Courier New";font-weight:400}.c29{font-weight:400;font-size:20pt;font-family:"Arial"}.c9{font-size:14pt;font-family:"Open Sans";font-weight:400}.c22{font-weight:400;font-size:11pt;font-family:"Open Sans"}.c30{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c14{padding:0;margin:0}.c16{color:inherit;text-decoration:inherit}.c23{margin-left:45pt;padding-left:-9pt}.c19{color:#1155cc;text-decoration:underline}.c25{vertical-align:baseline;font-style:normal}.c28{background-color:#fff2cc}.c18{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c30"><div><p class="c4 c18"><span class="c11 c13"></span></p></div><p class="c20 title" id="h.4v9lfldn86u"><span class="c10">Tutorial: Typing in the air using Chrome, depth camera and WebGL transform feedback</span></p><p class="c4 c18"><span class="c11 c13"></span></p><p class="c2"><span class="c9 c19"><a class="c16" href="mailto:aleksandar.stojiljkovic@intel.com">aleksandar.stojiljkovic@intel.com</a></span></p><p class="c0"><span class="c1"></span></p><p class="c2"><span class="c1 c28">Note: this tutorial is work in progress and the final version is expected to be published on 01.org in following days.</span></p><p class="c0"><span class="c1"></span></p><h1 class="c27" id="h.csfaz1ff6j0f"><span class="c11 c29">Introduction</span></h1><p class="c0"><span class="c1"></span></p><p class="c2"><span class="c1">When I showed this first to my friend Babu, he said something on the line of &ldquo;that&rsquo;s not convenient&rdquo;. Well, though I can type on it faster than scrolling through the character grid, it is correct - it is not convenient, but it is a good illustration for a tutorial.</span></p><p class="c0"><span class="c1"></span></p><p class="c2"><span class="c9">Few words about the setup first. Plug the</span><span class="c9">&nbsp;</span><span class="c9 c19 c25"><a class="c16" href="https://www.google.com/url?q=https://www.google.fi/url?sa%3Dt%26rct%3Dj%26q%3D%26esrc%3Ds%26source%3Dweb%26cd%3D1%26cad%3Drja%26uact%3D8%26ved%3D0ahUKEwim6rSGq6vUAhVsKpoKHWMjC9gQFggmMAA%26url%3Dhttps%253A%252F%252Fwww.intel.com%252Fcontent%252Fwww%252Fus%252Fen%252Farchitecture-and-technology%252Frealsense-overview.html%26usg%3DAFQjCNHYWDdHfpJ3LVnCZy2TlGRFv3SU-A%26sig2%3DjZlei6KsQJDean6847JFwA&amp;sa=D&amp;ust=1496829842600000&amp;usg=AFQjCNE-5uCzt4UNWPur64sE-C3NR3yo_Q">Intel&reg; RealSense&trade;</a></span></p><p class="c2"><span class="c9">&nbsp;</span><span class="c1">SR300 to USB 3.0 port of your Linux/Windows or Chrome OS machine. As a near-range camera, SR300 fits well for the use case here. The camera should point towards you, like in the photo. Once you get the hands closer to the camera, you&rsquo;ll notice they become visible over keyboard and then the closest fingertip movement is analyzed; if there is down-up movement and what is the key pressed.</span></p><p class="c26"><span class="c9">&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 558.67px; height: 419.00px;"><img alt="" src="images/image4.jpg" style="width: 558.67px; height: 419.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1"></span></p><p class="c2"><span class="c9">The approach could be improved, but that would be </span><span class="c9 c19"><a class="c16" href="#h.5nc1qzab4rr0">out of scope</a></span><span class="c1">&nbsp;of this tutorial.</span></p><p class="c0"><span class="c1"></span></p><p class="c2"><span class="c1">Eventually, you&rsquo;ll manage to type with not that many mistakes. Use the Delete key to fix them; this is the reason why I made it a bit larger and easier to hit. The captured screenshot animation shows how it works.</span></p><p class="c0"><span class="c1"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 356.00px;"><img alt="" src="images/image5.gif" style="width: 624.00px; height: 356.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c1"></span></p><h1 class="c27" id="h.e582kwj3ault"><span class="c11 c29">The algorithm and the API used</span></h1><p class="c0"><span class="c1"></span></p><p class="c2"><span class="c1">The approach has two steps; pre-process every pixel on GPU and identify potential candidates and then process them on CPU. The algorithms are expected to be highly tailored for the use cases. The algorithm split could be explained like this:</span></p><p class="c0"><span class="c1"></span></p><p class="c2"><span class="c9">GPU:</span></p><ul class="c14 lst-kix_mkjo0yq6ldhg-0 start"><li class="c2 c23"><span class="c9">Process every pixel or tile.</span></li><li class="c2 c23"><span class="c9">Map depth to color.</span></li><li class="c2 c23"><span class="c1">Sample around both depth and color texture, try to recognize features in shader.</span></li><li class="c2 c23"><span class="c9">Prepare the output result, either as transform feedback output or render to texture followed by readPixels.</span></li></ul><p class="c21"><span class="c9">CPU: </span></p><ul class="c14 lst-kix_mkjo0yq6ldhg-0"><li class="c7 c23"><span class="c1">After selecting interest area, handle the results.</span></li><li class="c2 c23"><span class="c1">It is essential that GPU (shader code) reduces number of candidates or the area to post-process on CPU, but that it is still robust enough to avoid missing the feature.</span></li></ul><p class="c15"><span class="c9">WebGL API used for this is presented on the picture.</span></p><p class="c17"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 560.00px; height: 400.71px;"><img alt="" src="images/image2.png" style="width: 560.00px; height: 400.71px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c11 c22"></span></p><p class="c4"><span class="c9">The part using video and texImage2D is described in </span><span class="c9 c19"><a class="c16" href="https://www.google.com/url?q=https://01.org/chromium/blogs/astojilj/2017/depth-camera-capture-html5&amp;sa=D&amp;ust=1496829842614000&amp;usg=AFQjCNHNdbtD9VU_64tSfprA4Vu7mmywzA">previous tutorial</a></span><span class="c1">. In short, we follow this steps:</span></p><p class="c15"><span class="c9">1. Create HTML </span><span class="c3">&lt;video&gt;</span><span class="c1">&nbsp;tag.</span></p><p class="c7"><span class="c9">2. Call </span><span class="c3">getUserMedia(constraint_to_depth_stream)</span><span class="c1">&nbsp;to get the depth stream. If algorithm requires it, get the color stream, too.</span></p><p class="c7"><span class="c9">3. Set the stream as video source, e.g. </span><span class="c3">video</span><span class="c3 c11">.srcObject = stream;</span></p><p class="c7"><span class="c1">4. Upload the latest captured depth video frame to texture, e.g.</span></p><p class="c7"><span class="c24 c3">gl.texImage2D(gl.TEXTURE_2D, 0, gl.R32F, gl.RED, gl.FLOAT, video);</span></p><p class="c0"><span class="c1"></span></p><h1 class="c27" id="h.bekywhr9akga"><span class="c11 c29">Step 1: GPU part of algorithm</span></h1><p class="c2"><span class="c1">This tutorial is describing transform feedback path. In the example we follow here, vertex shader code detects the points that are the centers of the area, as described by the picture:</span></p><p class="c0"><span class="c1"></span></p><p class="c26"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 376.00px; height: 286.00px;"><img alt="" src="images/image3.png" style="width: 376.00px; height: 286.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c1"></span></p><p class="c2"><span class="c9">So, we sample around and increase the distance of samples to the point. The idea is that on distance D (the green dots on the picture), all of the samples are inside the finger area, but on the distance D + 3, three or four out of four samples (the red dots) are outside the area.</span></p><p class="c21"><span class="c1">The part of vertex shader code doing this is:</span></p><p class="c0"><span class="c11 c12"></span></p><p class="c2"><span class="c11 c12">// Vertex shader code; transform feedback returns |depth|.</span></p><p class="c2"><span class="c12">// We have previously checked that the point is at least 3</span></p><p class="c2"><span class="c12">// pixels away from the edge, so start from i = 4.0. </span></p><p class="c2"><span class="c12">float i = 4.0;</span></p><p class="c2"><span class="c12">float number_of_dots_inside = 4.0;</span></p><p class="c0"><span class="c11 c12"></span></p><p class="c2"><span class="c12">for (; i &lt; MAX_DISTANCE; i += 3.0) {</span></p><p class="c2"><span class="c12">&nbsp; // Sample the texels like on the picture on the left.</span></p><p class="c2"><span class="c12">&nbsp; d_0 = texture(s_depth, depth_tex_pos + vec2(i, 0.0) * step).r;</span></p><p class="c2"><span class="c12">&nbsp; d_90 = texture(s_depth, depth_tex_pos + vec2(0.0, i) * step).r;</span></p><p class="c2"><span class="c12">&nbsp; d_180 = texture(s_depth, depth_tex_pos - vec2(i, 0.0) * step).r;</span></p><p class="c2"><span class="c12">&nbsp; d_270 = texture(s_depth, depth_tex_pos - vec2(0.0, i) * step).r;</span></p><p class="c2"><span class="c12">&nbsp; if (d_0 * d_90 * d_180 * d_270 == 0.0) {</span></p><p class="c2"><span class="c12">&nbsp; &nbsp; number_of_dots_inside = sign(d_0) + sign(d_90) +</span></p><p class="c2"><span class="c12">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sign(d_180) + sign(d_270);</span></p><p class="c2"><span class="c12">&nbsp; &nbsp; break;</span></p><p class="c2"><span class="c12">&nbsp; }</span></p><p class="c2"><span class="c12">}</span></p><p class="c0"><span class="c11 c12"></span></p><p class="c2"><span class="c11 c12">// &gt; 7.0 serves to eliminate &quot;thin&quot; areas. We pass depth &gt; 1.0 through</span></p><p class="c2"><span class="c11 c12">// transform feedback, so that CPU side of algorithm would understands</span></p><p class="c2"><span class="c12">// that this point is &quot;center of fingertip&quot; point and process it further.</span></p><p class="c2"><span class="c12">if (number_of_dots_inside &lt;= 1.0 &amp;&amp; i &gt; MIN_DISTANCE) {</span></p><p class="c2"><span class="c12">&nbsp; // Found it! Pack also the distance in the returned value.</span></p><p class="c2"><span class="c12">&nbsp; depth = i + depth;</span></p><p class="c2"><span class="c12">}</span></p><p class="c0"><span class="c24 c9"></span></p><h1 class="c27" id="h.sswzd2xz5yu"><span>Step 2: CPU part of algorithm</span></h1><p class="c4"><span class="c9">We start this by getting the transform feedback buffer data. The code includes the calls issuing the </span><span class="c9 c19"><a class="c16" href="#h.bekywhr9akga">Step 1</a></span><span class="c9">&nbsp;</span><span class="c1">and getting the buffer data, using getBufferSubData, and looks like:</span></p><p class="c4 c18"><span class="c1"></span></p><p class="c4"><span class="c11 c12">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gl.bindTransformFeedback(gl.TRANSFORM_FEEDBACK, gl.transform_feedback)</span></p><p class="c4"><span class="c11 c12">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gl.bindBufferBase(gl.TRANSFORM_FEEDBACK_BUFFER, 0, gl.tf_bo)</span></p><p class="c4"><span class="c11 c12">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gl.beginTransformFeedback(gl.POINTS);</span></p><p class="c4"><span class="c11 c12">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gl.drawArrays(gl.POINTS, 0, tf_output.length);</span></p><p class="c4"><span class="c11 c12">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gl.endTransformFeedback();</span></p><p class="c4"><span class="c11 c12">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gl.bindBufferBase(gl.TRANSFORM_FEEDBACK_BUFFER, 0, null)</span></p><p class="c4"><span class="c11 c12">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gl.disable(gl.RASTERIZER_DISCARD);</span></p><p class="c4"><span class="c11 c12">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gl.bindBuffer(gl.TRANSFORM_FEEDBACK_BUFFER, gl.tf_bo);</span></p><p class="c4"><span class="c11 c12">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gl.getBufferSubData(gl.TRANSFORM_FEEDBACK_BUFFER, 0, tf_output, 0, tf_output.length);</span></p><p class="c4"><span class="c12">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gl.bindBuffer(gl.TRANSFORM_FEEDBACK_BUFFER, null);</span></p><p class="c4 c18"><span class="c1"></span></p><p class="c4"><span class="c1">After that, on CPU side, we:</span></p><p class="c4"><span class="c1">1. attempt to compensate for the noise and identify the fingertip closest to the camera,</span></p><p class="c4"><span class="c1">2. pass the position of the fingertip to the shader rendering it,</span></p><p class="c4"><span class="c1">3. find the keyboard key under the fingertip and display it as hovered, </span></p><p class="c4"><span class="c1">4. detect press-down-and-up gesture of the single fingertip and</span></p><p class="c4"><span class="c1">5. issue a key click if detecting press-down-and-up gesture.</span></p><p class="c4 c18"><span class="c1"></span></p><p class="c4"><span class="c9">Let&rsquo;s start with the data we get from GPU (</span><span class="c9 c19"><a class="c16" href="#h.bekywhr9akga">Step 1</a></span><span class="c1">). White dots are identified as centers of the area, fingertip candidates. The red dot is the one among them that is the closest to the camera.</span></p><p class="c17"><span class="c9">&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 418.67px; height: 397.01px;"><img alt="" src="images/image1.gif" style="width: 418.67px; height: 397.01px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1"></span></p><p class="c4"><span class="c1">In the CPU side step, we take only that one, the red dot, and try to further stabilize it by calculating the center of mass (this would be the yellow dot on the pictures) of the shape around it. This step helps in reducing the noise, that is intrinsic to the infrared based depth sensing camera technology. Roughly speaking, the yellow dot is then the calculated center of mass of all connected points to the red point. When the finger is not moving, the yellow dot is more stable than the red, like on the picture.</span></p><p class="c17"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 374.67px; height: 343.26px;"><img alt="" src="images/image6.gif" style="width: 374.67px; height: 343.26px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c1"></span></p><p class="c4"><span class="c1">The algorithm implementing this is given in extractMinumums() function. Starting from the red dot, we enumerate the surrounding points on the same distance, as if spreading the waves of concentrical circles. For each point of the circle, we access the elements that is towards the center (the red point) to check if the point is connected to the red point. This way, we enumerate all the connected points to the red and calculate the average coordinate (i.e. the center of mass).</span></p><p class="c4 c18"><span class="c1"></span></p><h1 class="c31" id="h.5nc1qzab4rr0"><span class="c11 c29">Summary</span></h1><p class="c4"><span class="c1">The approach could be improved by tracking all of the fingers; not only that it would enable simultaneous key presses, but the click detection would be more robust as we would not only analyze the single closest point to to camera. Instead, it might make more sense to spend some time on different gesture recognition, e.g. low latency hand gesture click made of quick contact of thumb and pointing finger and try to incorporate it in a game. The next tutorial should be about it.</span></p><p class="c4 c18"><span class="c1"></span></p></body></html>